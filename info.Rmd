
<!-- ### Contact -->

<!-- `r app_contact_person` [smackie@its.jnj.com]`r paste0('(mailto:',app_contact_person_email,')')` -->

<!-- ### App version  -->

<!-- App version: `r app_version` -->
<!-- App version release date: `r app_release_date` -->

<!-- ### Objectives of why the App was built and what it does -->

<!-- ### Who the target users of the app are?  -->

<!-- ### What the key inputs/outputs of the app are? -->

<!-- ### Assumptions of the model (if applicable) -->

<!-- ### An Acknowledgments Section -->

<!-- ### Confidentiality Statement -->



# User Guide: Dataset Comparison Tool

### Contact

`r app_contact_person` [smackie@its.jnj.com]`r paste0('(mailto:',app_contact_person_email,')')`

### App version

App version: `r app_version`
App version release date: `r app_release_date`

## Purpose

This application has been built to compare ADPP and ADPP-like datasets and produce a comparison report of the two datasets. An interactive report detailing the differences is generated which can be downloaded as HTML and saved for reference if needed. The report can assist is demonstrating that the ADPP and ADPP-like datasets are equivalent and the same statistical outputs would be generated from either dataset.

## How to use

Simply upload the datasets and press **Compare Datasets** to generate a comparison report. There are two additional options that can be selected before generating the report.

1. **Include folder paths for the report?** - For traceability, it can be useful to include the original folder paths of the uploaded datasets in the report. However, due to technical limitations, the app cannot detect the folder paths automatically after file upload. If youâ€™d like to include this information, tick this box â€” two text fields will appear for you to enter the folder paths manually.

2. **Define unique keys for row-level checks?** To enable detailed row-level comparisons, you must define **unique key variables** â€” a combination of variables that uniquely identifies each row in the dataset. These must be consistent across both datasets. Common examples include USUBJID combined with a `--SEQ` variable (e.g., `PCSEQ`, `ASEQ`). If your datasets use parameter codes like `PARAMCD` or `PPTESTCD`, you may also include these to improve clarity in the comparison report. The app will validate the keys, and an error will be shown if they do not uniquely identify rows.

You can add comments to the report once it is generated which will be displayed in the app and included in the downloadable HTML report. This allows you to annotate findings or record decisions made during review.

The following conditions must be met before a report can be generated.

* **Unique Key Validation** If unique keys are defined then they must exist in each dataset and uniquely define each row in both datasets.
* **Dataset Validation** Each dataset must contain a `PARAMCD` and `AVAL` variable.

## Details of checks performed

The application compares two uploaded datasets by performing a series of automated checks, grouped into two categories:

- **Structure and Content Checks** â€” Basic dataset shape and format comparisons  
- **Row-Level Checks** â€” Detailed value comparisons (enabled only when unique keys are defined)

---

### Structure and Content Checks

These checks run on all datasets, even without key variables.

#### âœ… Differences in Number of Rows

Compares total row counts between the two datasets.

#### âœ… Differences in Columns

Identifies columns that are present in only one dataset.

#### âœ… Differences in Column Types

Lists shared variables that have different underlying data types (e.g., `numeric` vs `character`).

#### âœ… Differences in Rounding of AVAL

This check compares the **maximum number of decimal places** found in `AVAL` within each `PARAMCD` group. It is useful for identifying cases where `AVAL` values may have been rounded in one dataset but not the other.

To avoid false positives caused by floating-point precision differences, the check only considers the **first 10 decimal places** of each value.

For example, if `AVAL` is unrounded in one dataset and rounded to 3 decimal places in the other, the difference will be detected and tabulated here.

---

### Row-Level Checks

These checks are only available when **unique keys** have been defined and validated.

#### âœ… Unmatched Records

Shows which records appear in one dataset but not the other, based on the selected key variables.

#### âœ… AVAL Comparison

Compares the `AVAL` field between matched rows in both datasets. Includes two components:

##### AVAL Differences Summary

- Shows the total number of differing `AVAL` values
- Breaks them down by magnitude of difference:
  - âˆ† â‰¥ 1e-3
  - 1e-3 > âˆ† â‰¥ 1e-6
  - 1e-6 > âˆ† â‰¥ 1e-9
  - âˆ† â‰¤ 1e-9 (effectively negligible)
- Also reports the number of cases where `AVAL` is missing in one dataset

##### AVAL Differences Table

Displays every individual row-level `AVAL` mismatch for detailed inspection.

---

## ğŸ—ï¸ Unique Key Validation

Before row-level checks can run, the app verifies that your selected key variables:
- Exist in both datasets
- Uniquely identify each row in both datasets

If the validation fails, row-level comparisons will be disabled and a warning will appear.

---

---
