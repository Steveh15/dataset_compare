
<!-- ### Contact -->

<!-- `r app_contact_person` [smackie@its.jnj.com]`r paste0('(mailto:',app_contact_person_email,')')` -->

<!-- ### App version  -->

<!-- App version: `r app_version` -->
<!-- App version release date: `r app_release_date` -->

<!-- ### Objectives of why the App was built and what it does -->

<!-- ### Who the target users of the app are?  -->

<!-- ### What the key inputs/outputs of the app are? -->

<!-- ### Assumptions of the model (if applicable) -->

<!-- ### An Acknowledgments Section -->

<!-- ### Confidentiality Statement -->





### Contact

`r app_contact_person` [smackie@its.jnj.com]`r paste0('(mailto:',app_contact_person_email,')')`

### App version

App version: `r app_version`
App version release date: `r app_release_date`


#### Purpose

This application has been built to compare ADPP and ADPP-like datasets. Two datasets are input and compared for key differences. An interactive report detailing the differences is generated which can be downloaded as HTML and saved for reference if needed.

# 🧪 User Guide: Dataset Comparison Tool

## Overview

The application compares two uploaded datasets by performing a series of automated checks, grouped into two categories:

- **Structure and Content Checks** — Basic dataset shape and format comparisons  
- **Row-Level Checks** — Detailed value comparisons (enabled only when unique keys are defined)

To unlock row-level comparisons, define **unique key variables** that uniquely identify each row in the dataset. Common combinations include `USUBJID` and a `--SEQ` variable (e.g., `PCSEQ` or `ASEQ`). If the parameter variables (e.g. `PPTESTCD` or `PARAMCD`) are the same in each dataset then you may also find it useful to include on of these, as this will help identify the differences shown in the report.

Once datasets are uploaded and (optionally) key variables are defined, click **Compare Datasets** to run the comparison. Results appear in the main panel and can be downloaded as an interactive HTML report.

You can also attach comments to individual checks. These will be saved and included in the report when downloaded.

---

## Structure and Content Checks

These checks run on all datasets, even without key variables.

### ✅ Differences in Number of Rows

Compares total row counts between the two datasets.

### ✅ Differences in Columns

Identifies columns that are present in only one dataset.

### ✅ Differences in Column Types

Lists shared variables that have different underlying data types (e.g., `numeric` vs `character`).

### ✅ Differences in Rounding of AVAL

This check compares the **maximum number of decimal places** found in `AVAL` within each `PARAMCD` group. It is useful for identifying cases where `AVAL` values may have been rounded in one dataset but not the other.

To avoid false positives caused by floating-point precision differences, the check only considers the **first 10 decimal places** of each value.

For example, if `AVAL` is unrounded in one dataset and rounded to 3 decimal places in the other, the difference will be detected and tabulated here.

---

## Row-Level Checks

These checks are only available when **unique keys** have been defined and validated.

### ✅ Unmatched Records

Shows which records appear in one dataset but not the other, based on the selected key variables.

### ✅ AVAL Comparison

Compares the `AVAL` field between matched rows in both datasets. Includes two components:

#### AVAL Differences Summary

- Shows the total number of differing `AVAL` values
- Breaks them down by magnitude of difference:
  - ∆ ≥ 1e-3
  - 1e-3 > ∆ ≥ 1e-6
  - 1e-6 > ∆ ≥ 1e-9
  - ∆ ≤ 1e-9 (effectively negligible)
- Also reports the number of cases where `AVAL` is missing in one dataset

#### AVAL Differences Table

Displays every individual row-level `AVAL` mismatch for detailed inspection.

---

## 🗝️ Unique Key Validation

Before row-level checks can run, the app verifies that your selected key variables:
- Exist in both datasets
- Uniquely identify each row in both datasets

If the validation fails, row-level comparisons will be disabled and a warning will appear.

---

## 💬 Comments

Each check allows you to add a **comment**, which will be displayed in the app and included in the downloadable HTML report. This allows you to annotate findings or record decisions made during review.

---
